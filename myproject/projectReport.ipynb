{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5754fb91",
   "metadata": {},
   "source": [
    "1: Scrapy Crawler  \n",
    "<small> Done with assistance from ChatGPT (bug fixing and explanation)  \n",
    "Scrapy Tutorial — Scrapy 2.3.0 documentation. (n.d.). Docs.scrapy.org. https://docs.scrapy.org/en/latest/intro/tutorial.html </small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy, os\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class HtmlCrawler(scrapy.Spider):\n",
    "    name = \"html_crawler\"\n",
    "    # initialize crawler\n",
    "    def __init__(self, seed=\"https://quotes.toscrape.com/\", max_pages=20, max_depth=2, *args, **kwargs):\n",
    "        super(HtmlCrawler, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = [seed]\n",
    "        self.allowed_domain = urlparse(seed).netloc\n",
    "        self.max_pages = int(max_pages)\n",
    "        self.max_depth = int(max_depth)\n",
    "        self.page_count = 0\n",
    "        self.link_extractor = LinkExtractor(allow_domains=[self.allowed_domain])\n",
    "\n",
    "    def parse(self, response):\n",
    "        # stop if reached max pages\n",
    "        if self.page_count >= self.max_pages:\n",
    "            self.crawler.engine.close_spider(self, reason='max pages reached')\n",
    "            return\n",
    "\n",
    "        # increment and get page url and text\n",
    "        self.page_count += 1\n",
    "        pageURL = response.url\n",
    "        pageHTML = response.text\n",
    "\n",
    "        # save content in file\n",
    "        filename = f\"downloaded_page_{self.page_count}.html\"\n",
    "        path = os.path.join(\"../\", filename)\n",
    "\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(pageHTML)\n",
    "        self.logger.info(f\"Saved {pageURL} as {filename}\")\n",
    "\n",
    "        # get next link if within depth\n",
    "        currDepth = response.meta.get('depth', 0)\n",
    "        if currDepth < self.max_depth:\n",
    "            for link in self.link_extractor.extract_links(response):\n",
    "                yield scrapy.Request(link.url, callback=self.parse, meta={'depth': currDepth + 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a480ba",
   "metadata": {},
   "source": [
    "2: Scikit-learn Indexer  \n",
    "<small> Done with assistance from ChatGPT (bug fixing and explanation)  \n",
    "Examples. (2025). Scikit-Learn. https://scikit-learn.org/stable/auto_examples/  \n",
    "Working With Text Data. (2024). Scikit-Learn. https://scikit-learn.org/1.4/tutorial/text_analytics/working_with_text_data.html </small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project assisted and partially generated using the assistance of LLM ChatGPT\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class SklearnIndexer:\n",
    "    def __init__(self, input_folder=\".\", output_file=\"./inverted_index.json\"):\n",
    "            self.input_folder = input_folder\n",
    "            self.output_file = output_file\n",
    "            self.documents = []\n",
    "            self.doc_ids = []\n",
    "            self.vectorizer = None\n",
    "            self.tfidf_matrix = None\n",
    "            self.feature_names = None\n",
    "\n",
    "    # html to plaintext\n",
    "    def html_to_text(self, html):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        text = soup.get_text(separator=\" \", strip=True)\n",
    "        splitText = re.sub(r\"\\s+\", \" \", text)\n",
    "        return splitText\n",
    "\n",
    "    # load the downloaded files\n",
    "    def load_documents(self):\n",
    "        files = glob.glob(os.path.join(self.input_folder, \"downloaded_page_*.html\"))\n",
    "\n",
    "        for path in files:\n",
    "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                html = f.read()\n",
    "\n",
    "            text = self.html_to_text(html)\n",
    "\n",
    "            self.documents.append(text)\n",
    "            self.doc_ids.append(os.path.basename(path))\n",
    "\n",
    "        print(f\"Loaded {len(self.documents)} documents.\")\n",
    "\n",
    "    # build tfidf matrix\n",
    "    def build_tfidf(self):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            lowercase=True,\n",
    "        )\n",
    "\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(self.documents)\n",
    "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "\n",
    "    # build the inverted index\n",
    "    def build_inverted_index(self):\n",
    "        inverted = {}\n",
    "\n",
    "        rows, cols = self.tfidf_matrix.nonzero()\n",
    "\n",
    "        for row, col in zip(rows, cols):\n",
    "            term = self.feature_names[col]\n",
    "            score = float(self.tfidf_matrix[row, col])\n",
    "            docID = self.doc_ids[row]\n",
    "\n",
    "            if term not in inverted:\n",
    "                inverted[term] = []\n",
    "\n",
    "            inverted[term].append({\n",
    "                \"doc_id\": docID,\n",
    "                \"tfidf\": score\n",
    "            })\n",
    "\n",
    "        return inverted\n",
    "\n",
    "    # save index to a json file\n",
    "    def save_index(self, inverted_index):\n",
    "        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)\n",
    "\n",
    "        with open(self.output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(inverted_index, f, indent=4)\n",
    "\n",
    "        print(f\"Saved index at {self.output_file}\")\n",
    "\n",
    "    # load docs, build tfidf, build index\n",
    "    def run(self):\n",
    "        self.load_documents()\n",
    "        self.build_tfidf()\n",
    "        inverted = self.build_inverted_index()\n",
    "        self.save_index(inverted)\n",
    "\n",
    "    # find based on cosine similarity\n",
    "    def search(self, query, top_k=5):\n",
    "        # create query vec and find cosine similarity\n",
    "        queryVec = self.vectorizer.transform([query])\n",
    "        scores = cosine_similarity(queryVec, self.tfidf_matrix)[0]\n",
    "        \n",
    "        # return sorted by rank\n",
    "        ranked = sorted(zip(self.doc_ids, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return ranked[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45d30d",
   "metadata": {},
   "source": [
    "Run Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9653325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexer import SklearnIndexer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    indexer = SklearnIndexer(input_folder=\".\", output_file=\"./inverted_index.json\")\n",
    "    indexer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872a1f5",
   "metadata": {},
   "source": [
    "Example Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf849359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"tea\": [\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_1.html\",\n",
    "            \"tfidf\": 0.06385495787246827\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_10.html\",\n",
    "            \"tfidf\": 0.10246253140246371\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_15.html\",\n",
    "            \"tfidf\": 0.1406372739365191\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_3.html\",\n",
    "            \"tfidf\": 0.09926167534880873\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_9.html\",\n",
    "            \"tfidf\": 0.13222187825641707\n",
    "        }\n",
    "    ],\n",
    "    \"bag\": [\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_1.html\",\n",
    "            \"tfidf\": 0.08350233517450086\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_15.html\",\n",
    "            \"tfidf\": 0.18390961606661183\n",
    "        }\n",
    "    ],\n",
    "    \"know\": [\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_1.html\",\n",
    "            \"tfidf\": 0.09875103357053636\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_11.html\",\n",
    "            \"tfidf\": 0.04263865531418255\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_12.html\",\n",
    "            \"tfidf\": 0.18689371662278184\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_13.html\",\n",
    "            \"tfidf\": 0.12017676879799435\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_14.html\",\n",
    "            \"tfidf\": 0.03329951675649862\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_15.html\",\n",
    "            \"tfidf\": 0.10874704660765194\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_2.html\",\n",
    "            \"tfidf\": 0.12017676879799435\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_7.html\",\n",
    "            \"tfidf\": 0.060310442047078756\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"downloaded_page_9.html\",\n",
    "            \"tfidf\": 0.10223988530801692\n",
    "        }\n",
    "    ],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453128e",
   "metadata": {},
   "source": [
    "3: Flask Application and Query Handling  \n",
    "<small> Done with assistance from ChatGPT (bug fixing and explanation) \n",
    "Flask. (2010). Welcome to Flask — Flask Documentation (3.0.x). Palletsprojects.com. https://flask.palletsprojects.com/en/stable/ \n",
    "NLTK :: nltk. (n.d.). Www.nltk.org. https://www.nltk.org/_modules/nltk.html </small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c35b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project assisted and partially generated using the assistance of LLM ChatGPT\n",
    "from flask import Flask, request, Response\n",
    "import csv\n",
    "from indexer import SklearnIndexer\n",
    "import os\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "app = Flask(__name__)\n",
    "INDEX_FILE = os.path.join(\".\", \"inverted_index.json\")\n",
    "DOC_FOLDER = \".\"\n",
    "\n",
    "indexer = SklearnIndexer(input_folder=DOC_FOLDER, output_file=INDEX_FILE)\n",
    "VOCAB = set()\n",
    "\n",
    "# run indexer if not existant\n",
    "if not os.path.exists(INDEX_FILE):\n",
    "    print(\"Inverted index not found, running indexer\")\n",
    "    indexer.run()\n",
    "    VOCAB = set(indexer.vectorizer.get_feature_names_out())\n",
    "else:\n",
    "    print(\"Loading existing index\")\n",
    "    indexer.load_documents()\n",
    "    indexer.build_tfidf()\n",
    "    VOCAB = set(indexer.vectorizer.get_feature_names_out())\n",
    "\n",
    "# spelling correction using edit distance\n",
    "def spelling_check(query, vocab, max_distance=2):\n",
    "    tokens = query.lower().split()\n",
    "    suggestions = {}\n",
    "\n",
    "    # go through each token and find closest word using edit distance\n",
    "    for t in tokens:\n",
    "        if len(t) <= 2:\n",
    "            continue\n",
    "\n",
    "        closestWord = None\n",
    "        closestDist = 999\n",
    "\n",
    "        for v in vocab:\n",
    "            d = edit_distance(t, v)\n",
    "            if d < closestDist:\n",
    "                closestDist = d\n",
    "                closestWord = v\n",
    "            if closestDist == 0:\n",
    "                break\n",
    "\n",
    "        if closestDist <= max_distance and closestWord != t:\n",
    "            suggestions[t] = closestWord\n",
    "\n",
    "    return suggestions if suggestions else None\n",
    "\n",
    "# format output\n",
    "def format_output(query, data):\n",
    "    lines = []\n",
    "    lines.append(f\"\\nQuery: {query}\")\n",
    "    lines.append(\"Results:\")\n",
    "    for item in data.get(\"results\", []):\n",
    "        lines.append(f\"  {item['document']} -> {item['score']}\")\n",
    "\n",
    "    suggestions = data.get(\"suggestions\")\n",
    "    if suggestions:\n",
    "        lines.append(\"Suggestions:\")\n",
    "        for wrong, correct in suggestions.items():\n",
    "            lines.append(f\"  {wrong} -> {correct}\")\n",
    "    else:\n",
    "        lines.append(\"Suggestions: None\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "@app.route(\"/query\", methods=[\"POST\"])\n",
    "def query_processor():\n",
    "    if 'file' not in request.files:\n",
    "        return Response(\"ERROR: No file provided\", status=400)\n",
    "\n",
    "    file = request.files['file']\n",
    "\n",
    "    if not file.filename.endswith(\".csv\"):\n",
    "        return Response(\"ERROR: Only CSV files allowed\", status=400)\n",
    "\n",
    "    decoded = file.read().decode(\"utf-8\").splitlines()\n",
    "    reader = csv.DictReader(decoded)\n",
    "\n",
    "    if 'query' not in reader.fieldnames:\n",
    "        return Response(\"ERROR: CSV must contain a 'query' column\", status=400)\n",
    "\n",
    "    queries = []\n",
    "    for row in reader:\n",
    "        text = row['query'].strip()\n",
    "        if text:\n",
    "            queries.append(text)\n",
    "\n",
    "    if not queries:\n",
    "        return Response(\"ERROR: No valid queries found\", status=400)\n",
    "\n",
    "    # build text output\n",
    "    output_text = \"\"\n",
    "    for q in queries:\n",
    "        suggestion = spelling_check(q, VOCAB)\n",
    "        raw_results = indexer.search(q, top_k=5)\n",
    "\n",
    "        results = [\n",
    "            {\"document\": doc, \"score\": float(score)}\n",
    "            for doc, score in raw_results\n",
    "        ]\n",
    "\n",
    "        format_results = format_output(q, {\"results\": results, \"suggestions\": suggestion})\n",
    "        output_text += format_results + \"\\n\\n\"\n",
    "\n",
    "    return (output_text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842d5ad",
   "metadata": {},
   "source": [
    "Test Flask Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "csv_file_path = \"queries.csv\" \n",
    "url = \"http://127.0.0.1:5000/query\"\n",
    "\n",
    "with open(csv_file_path, \"rb\") as f:\n",
    "    files = {\"file\": f}\n",
    "    try:\n",
    "        response = requests.post(url, files=files)\n",
    "        response.raise_for_status()  \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "    else:\n",
    "        # Save returned text file\n",
    "        filename = \"results.txt\"\n",
    "        with open(filename, \"wb\") as out:\n",
    "            out.write(response.content)\n",
    "\n",
    "        print(f\"Saved output to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde7144",
   "metadata": {},
   "source": [
    "Query Search Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b801322",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query: quotes\n",
    "Results:\n",
    "  downloaded_page_12.html -> 0.29662660165228943\n",
    "  downloaded_page_7.html -> 0.2546243690774928\n",
    "  downloaded_page_8.html -> 0.2515624369648701\n",
    "  downloaded_page_5.html -> 0.2510613501789357\n",
    "  downloaded_page_2.html -> 0.2443986794990816\n",
    "Suggestions: None\n",
    "\n",
    "\n",
    "Query: stupid\n",
    "Results:\n",
    "  downloaded_page_12.html -> 0.20750141045778497\n",
    "  downloaded_page_10.html -> 0.15090770532077524\n",
    "  downloaded_page_11.html -> 0.10049849694766169\n",
    "  downloaded_page_1.html -> 0.05762524820913743\n",
    "  downloaded_page_17.html -> 0.050740502076156345\n",
    "Suggestions: None\n",
    "\n",
    "\n",
    "Query: world\n",
    "Results:\n",
    "  downloaded_page_1.html -> 0.12371606314979401\n",
    "  downloaded_page_16.html -> 0.10161315288356786\n",
    "  downloaded_page_13.html -> 0.06355193616720467\n",
    "  downloaded_page_19.html -> 0.047534820618943\n",
    "  downloaded_page_9.html -> 0.03420312693054097\n",
    "Suggestions: None\n",
    "\n",
    "\n",
    "Query: stuplld\n",
    "Results:\n",
    "  downloaded_page_1.html -> 0.0\n",
    "  downloaded_page_10.html -> 0.0\n",
    "  downloaded_page_11.html -> 0.0\n",
    "  downloaded_page_12.html -> 0.0\n",
    "  downloaded_page_13.html -> 0.0\n",
    "Suggestions:\n",
    "  stuplld -> stupid\n",
    "\n",
    "\n",
    "Query: lady\n",
    "Results:\n",
    "  downloaded_page_12.html -> 0.19429842272174835\n",
    "  downloaded_page_10.html -> 0.1413056858538812\n",
    "  downloaded_page_19.html -> 0.0829290690600552\n",
    "  downloaded_page_1.html -> 0.05395864447997341\n",
    "  downloaded_page_17.html -> 0.047511963893433366\n",
    "Suggestions: None\n",
    "\n",
    "\n",
    "Query: ladyy\n",
    "Results:\n",
    "  downloaded_page_1.html -> 0.0\n",
    "  downloaded_page_10.html -> 0.0\n",
    "  downloaded_page_11.html -> 0.0\n",
    "  downloaded_page_12.html -> 0.0\n",
    "  downloaded_page_13.html -> 0.0\n",
    "Suggestions:\n",
    "  ladyy -> lady\n",
    "\n",
    "\n",
    "Query: love\n",
    "Results:\n",
    "  downloaded_page_9.html -> 0.36624082650321055\n",
    "  downloaded_page_14.html -> 0.233642004676565\n",
    "  downloaded_page_4.html -> 0.11296637354168547\n",
    "  downloaded_page_12.html -> 0.10369968741464176\n",
    "  downloaded_page_7.html -> 0.08901584461544056\n",
    "Suggestions: None"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
