# Project assisted and partially generated using the assistance of LLM ChatGPT
import scrapy, os
from scrapy.linkextractors import LinkExtractor
from urllib.parse import urlparse

class HtmlCrawler(scrapy.Spider):
    name = "html_crawler"
    # initialize crawler
    def __init__(self, seed="https://quotes.toscrape.com/", max_pages=20, max_depth=2, *args, **kwargs):
        super(HtmlCrawler, self).__init__(*args, **kwargs)
        self.start_urls = [seed]
        self.allowed_domain = urlparse(seed).netloc
        self.max_pages = int(max_pages)
        self.max_depth = int(max_depth)
        self.page_count = 0
        self.link_extractor = LinkExtractor(allow_domains=[self.allowed_domain])

    def parse(self, response):
        # stop if reached max pages
        if self.page_count >= self.max_pages:
            self.crawler.engine.close_spider(self, reason='max pages reached')
            return

        # increment and get page url and text
        self.page_count += 1
        pageURL = response.url
        pageHTML = response.text

        # save content in file
        filename = f"downloaded_page_{self.page_count}.html"
        path = os.path.join("../", filename)

        with open(path, "w", encoding="utf-8") as f:
            f.write(pageHTML)
        self.logger.info(f"Saved {pageURL} as {filename}")

        # get next link if within depth
        currDepth = response.meta.get('depth', 0)
        if currDepth < self.max_depth:
            for link in self.link_extractor.extract_links(response):
                yield scrapy.Request(link.url, callback=self.parse, meta={'depth': currDepth + 1})